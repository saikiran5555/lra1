{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "910e7024",
   "metadata": {},
   "source": [
    "In logistic regression, the cost function used is typically the \"Logistic Loss\" or \"Log Loss,\" which is a type of \"Cross-Entropy Loss.\" This cost function is specifically designed for binary classification problems, although it can be extended to multi-class classification scenarios as well. Let's delve into the details:\n",
    "\n",
    "Logistic Regression Cost Function\n",
    "Binary Cross-Entropy Loss:\n",
    "\n",
    "For a binary classification problem (with outputs 0 or 1), the cost function is defined as:\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "−\n",
    "1\n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "[\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "log\n",
    "⁡\n",
    "(\n",
    "ℎ\n",
    "�\n",
    "(\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    ")\n",
    ")\n",
    "+\n",
    "(\n",
    "1\n",
    "−\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    ")\n",
    "log\n",
    "⁡\n",
    "(\n",
    "1\n",
    "−\n",
    "ℎ\n",
    "�\n",
    "(\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    ")\n",
    ")\n",
    "]\n",
    "J(θ)=− \n",
    "m\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "m\n",
    "​\n",
    " [y \n",
    "(i)\n",
    " log(h \n",
    "θ\n",
    "​\n",
    " (x \n",
    "(i)\n",
    " ))+(1−y \n",
    "(i)\n",
    " )log(1−h \n",
    "θ\n",
    "​\n",
    " (x \n",
    "(i)\n",
    " ))]\n",
    "Here, \n",
    "�\n",
    "m is the number of training examples, \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "y \n",
    "(i)\n",
    "  is the actual output, and \n",
    "ℎ\n",
    "�\n",
    "(\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    ")\n",
    "h \n",
    "θ\n",
    "​\n",
    " (x \n",
    "(i)\n",
    " ) is the predicted probability that \n",
    "�\n",
    "=\n",
    "1\n",
    "y=1 for input \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "x \n",
    "(i)\n",
    " . This predicted probability is given by the logistic function:\n",
    "ℎ\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "1\n",
    "1\n",
    "+\n",
    "�\n",
    "−\n",
    "�\n",
    "�\n",
    "�\n",
    "h \n",
    "θ\n",
    "​\n",
    " (x)= \n",
    "1+e \n",
    "−θ \n",
    "T\n",
    " x\n",
    " \n",
    "1\n",
    "​\n",
    " \n",
    "Intuition Behind the Cost Function:\n",
    "\n",
    "The cost function effectively penalizes the model for wrong predictions in a way that varies logarithmically with the predicted probability. If the actual class is 1 and the model predicts 0, the cost is high. Similarly, if the actual class is 0 and the model predicts 1, the cost again is high.\n",
    "Optimization of the Cost Function\n",
    "Gradient Descent:\n",
    "\n",
    "The parameters \n",
    "�\n",
    "θ of the model are optimized using an algorithm like gradient descent.\n",
    "In each iteration, \n",
    "�\n",
    "θ is updated as:\n",
    "�\n",
    "�\n",
    ":\n",
    "=\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "∂\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "∂\n",
    "�\n",
    "�\n",
    "θ \n",
    "j\n",
    "​\n",
    " :=θ \n",
    "j\n",
    "​\n",
    " −α \n",
    "∂θ \n",
    "j\n",
    "​\n",
    " \n",
    "∂J(θ)\n",
    "​\n",
    " \n",
    "Here, \n",
    "�\n",
    "α is the learning rate, and \n",
    "∂\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "∂\n",
    "�\n",
    "�\n",
    "∂θ \n",
    "j\n",
    "​\n",
    " \n",
    "∂J(θ)\n",
    "​\n",
    "  is the partial derivative of the cost function with respect to the parameter \n",
    "�\n",
    "�\n",
    "θ \n",
    "j\n",
    "​\n",
    " .\n",
    "Partial Derivative Calculation:\n",
    "\n",
    "The partial derivative of the cost function with respect to each parameter \n",
    "�\n",
    "�\n",
    "θ \n",
    "j\n",
    "​\n",
    "  reflects how \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "J(θ) changes with a small change in \n",
    "�\n",
    "�\n",
    "θ \n",
    "j\n",
    "​\n",
    " . For logistic regression, it turns out to be:\n",
    "∂\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "∂\n",
    "�\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "(\n",
    "ℎ\n",
    "�\n",
    "(\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    ")\n",
    "−\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    ")\n",
    "�\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "∂θ \n",
    "j\n",
    "​\n",
    " \n",
    "∂J(θ)\n",
    "​\n",
    " = \n",
    "m\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "m\n",
    "​\n",
    " (h \n",
    "θ\n",
    "​\n",
    " (x \n",
    "(i)\n",
    " )−y \n",
    "(i)\n",
    " )x \n",
    "j\n",
    "(i)\n",
    "​\n",
    " \n",
    "Convergence:\n",
    "\n",
    "The optimization process is repeated until convergence, meaning the changes in \n",
    "�\n",
    "θ become negligibly small, or a maximum number of iterations is reached.\n",
    "Regularization:\n",
    "\n",
    "To prevent overfitting, regularization terms (like L1 or L2 penalties) may be added to the cost function, which penalizes large values of the parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
